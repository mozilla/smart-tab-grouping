{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn"
      ],
      "metadata": {
        "id": "bvo5V8FKuRch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ask Vasish or Rolf for any other dataset (if needed)\n",
        "df = pd.read_csv('./synthetic_processed.csv')\n",
        "print(df['test_set_id'].nunique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUe1htY2uhG1",
        "outputId": "bb9172f7-22b4-4a02-dd1e-07357bf2da25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def generate_window_from_df(df):\n",
        "  # needs to return the tab titles, the urls and the group name\n",
        "  return {'titles': list(df['title']), 'urls': list(df['url']), 'group_name': list(df['task'])}\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "windows = []\n",
        "for test_set_id in df['test_set_id'].unique():\n",
        "  windows.append(generate_window_from_df(df[df['test_set_id'] == test_set_id]))"
      ],
      "metadata": {
        "id": "odDJcgs8ucs5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_pJseyGuKmr"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import (\n",
        "    KMeans, DBSCAN, AgglomerativeClustering, Birch, OPTICS, SpectralClustering\n",
        ")\n",
        "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from tqdm import tqdm\n",
        "import umap\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "sns.set_theme()\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------- Setup ----------\n",
        "reducers = {\n",
        "    'pca': PCA,\n",
        "    'svd': TruncatedSVD,\n",
        "    'umap': umap.UMAP\n",
        "}\n",
        "\n",
        "dims = ['raw'] + [f'{k}_{n}' for k in reducers for n in (5, 15)]\n",
        "methods = [\n",
        "    ('kmeans', KMeans, True),\n",
        "    ('agglo', AgglomerativeClustering, True),\n",
        "    ('birch', Birch, True),\n",
        "    ('spectral', SpectralClustering, True),\n",
        "    ('optics', OPTICS, False),\n",
        "    ('dbscan', DBSCAN, False),\n",
        "]\n",
        "\n",
        "results = {'window': []}\n",
        "timings = {f'{m}_{d}': [] for m, _, _ in methods for d in dims}\n",
        "dbscan_configs = {dim: [] for dim in dims}\n",
        "\n",
        "for method, _, _ in methods:\n",
        "    for dim in dims:\n",
        "        results[f'{method}_{dim}'] = []\n",
        "\n",
        "# ---------- Reduction Helper ----------\n",
        "def safe_reduce(X, reducer_cls, n_components):\n",
        "    n_samples = len(X)\n",
        "    if n_samples <= n_components or n_samples <= 2:\n",
        "        return None\n",
        "    try:\n",
        "        if reducer_cls is umap.UMAP:\n",
        "            reducer = reducer_cls(n_components=n_components, n_neighbors=min(15, n_samples - 1), random_state=42)\n",
        "        else:\n",
        "            reducer = reducer_cls(n_components=n_components, random_state=42)\n",
        "        return reducer.fit_transform(X)\n",
        "    except Exception as e:\n",
        "        print(f\"{reducer_cls.__name__} failed (n_components={n_components}): {e}\")\n",
        "        return None\n",
        "\n",
        "# ---------- Clustering Helpers ----------\n",
        "def run_kmeans_auto(X, true_labels):\n",
        "    best_score = -1\n",
        "    best_k = 2\n",
        "    max_k = min(len(X) - 1, 10)\n",
        "    for k in range(2, max_k + 1):\n",
        "        try:\n",
        "            model = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
        "            labels = model.fit_predict(X)\n",
        "            score = silhouette_score(X, labels)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_k = k\n",
        "        except:\n",
        "            continue\n",
        "    final_model = KMeans(n_clusters=best_k, n_init='auto', random_state=42)\n",
        "    predicted = final_model.fit_predict(X)\n",
        "    return adjusted_rand_score(true_labels, predicted)\n",
        "\n",
        "def run_fixed_cluster_method(X, method_cls, true_labels):\n",
        "    best_score = -1\n",
        "    best_k = 2\n",
        "    max_k = min(len(X) - 1, 10)\n",
        "    for k in range(2, max_k + 1):\n",
        "        try:\n",
        "            model = method_cls(n_clusters=k)\n",
        "            labels = model.fit_predict(X)\n",
        "            score = silhouette_score(X, labels)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_k = k\n",
        "        except:\n",
        "            continue\n",
        "    try:\n",
        "        final_model = method_cls(n_clusters=best_k)\n",
        "        labels = final_model.fit_predict(X)\n",
        "        return adjusted_rand_score(true_labels, labels)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def run_dbscan(X, true_labels, label):\n",
        "    best_ars = -1\n",
        "    best_cfg = \"skipped\"\n",
        "    for eps in [0.2, 0.3, 0.5]:\n",
        "        try:\n",
        "            db = DBSCAN(eps=eps, min_samples=2, metric='euclidean')\n",
        "            labels = db.fit_predict(X)\n",
        "            if len(set(labels)) > 1:\n",
        "                ars = adjusted_rand_score(true_labels, labels)\n",
        "                if ars > best_ars:\n",
        "                    best_ars = ars\n",
        "                    best_cfg = f\"eps={eps}\"\n",
        "        except:\n",
        "            continue\n",
        "    dbscan_configs[label].append(best_cfg)\n",
        "    return best_ars if best_ars != -1 else np.nan\n",
        "\n",
        "def run_optics(X, true_labels):\n",
        "    try:\n",
        "        model = OPTICS(metric='euclidean')\n",
        "        labels = model.fit_predict(X)\n",
        "        return adjusted_rand_score(true_labels, labels) if len(set(labels)) > 1 else np.nan\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "# ---------- Main Loop ----------\n",
        "for i, window in tqdm(enumerate(windows)):\n",
        "    titles = window.get('titles', [])\n",
        "    groups = window.get('group_name', [])\n",
        "\n",
        "    if len(titles) < 2 or len(set(groups)) <= 1:\n",
        "        print(f\"Window {i}: Skipped (not enough data or only one group)\")\n",
        "        continue\n",
        "\n",
        "    embeddings = model.encode(titles)\n",
        "    le = LabelEncoder()\n",
        "    true_labels = le.fit_transform(groups)\n",
        "\n",
        "    reduced_data = {'raw': embeddings}\n",
        "    for name, reducer in reducers.items():\n",
        "        for dim in (5, 15):\n",
        "            key = f'{name}_{dim}'\n",
        "            reduced_data[key] = safe_reduce(embeddings, reducer, dim)\n",
        "\n",
        "    results['window'].append(f\"Window {i}\")\n",
        "\n",
        "    for method_name, method_cls, supports_k in methods:\n",
        "        for dim_key, X in reduced_data.items():\n",
        "            start = time.time()\n",
        "            if X is None:\n",
        "                ars = np.nan\n",
        "            elif method_name == 'kmeans':\n",
        "                ars = run_kmeans_auto(X, true_labels)\n",
        "            elif method_name == 'dbscan':\n",
        "                ars = run_dbscan(X, true_labels, dim_key)\n",
        "            elif method_name == 'optics':\n",
        "                ars = run_optics(X, true_labels)\n",
        "            else:\n",
        "                ars = run_fixed_cluster_method(X, method_cls, true_labels)\n",
        "            elapsed = time.time() - start\n",
        "            results[f'{method_name}_{dim_key}'].append(ars)\n",
        "            timings[f'{method_name}_{dim_key}'].append(elapsed)\n",
        "\n",
        "# ---------- Save to CSV ----------\n",
        "df = pd.DataFrame(results)\n",
        "for col, times in timings.items():\n",
        "    df[f'{col}_time'] = times\n",
        "\n",
        "for dim in dims:\n",
        "    if dim in dbscan_configs:\n",
        "        padded_configs = dbscan_configs[dim] + ['skipped'] * (len(df) - len(dbscan_configs[dim]))\n",
        "        df[f'dbscan_{dim}_config'] = padded_configs\n",
        "df.to_csv('ars_scores_full.csv', index=False)\n",
        "print(\"\\nSaved full ARS comparison to 'ars_scores_full.csv'\")\n",
        "\n",
        "# ---------- Analysis ----------\n",
        "top_n = 50\n",
        "method_dims = [f'{m}_{d}' for m, _, _ in methods for d in dims]\n",
        "avg_scores = {col: df[col].mean(skipna=True) for col in method_dims}\n",
        "avg_times = {col: np.mean(timings.get(col, [np.nan])) for col in method_dims}\n",
        "top_methods = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "top_cols = [name for name, _ in top_methods]\n",
        "top_avgs = [score for _, score in top_methods]\n",
        "\n",
        "def get_color(col):\n",
        "    if 'raw' in col:\n",
        "        return 'gray'\n",
        "    elif 'pca' in col:\n",
        "        return 'steelblue'\n",
        "    elif 'svd' in col:\n",
        "        return 'seagreen'\n",
        "    elif 'umap' in col:\n",
        "        return 'darkorange'\n",
        "    return 'black'\n",
        "\n",
        "colors = [get_color(col) for col in top_cols]\n",
        "\n",
        "# ---------- Summary Table ----------\n",
        "summary_data = []\n",
        "for col in method_dims:\n",
        "    method, dim = col.split('_', 1)\n",
        "    score = avg_scores.get(col, np.nan)\n",
        "    avg_time = avg_times.get(col, np.nan)\n",
        "    param = dbscan_configs[dim][0] if method == 'dbscan' and len(dbscan_configs[dim]) > 0 else '-'\n",
        "    summary_data.append({\n",
        "        'Clustering Method': method.capitalize(),\n",
        "        'Dim Reduction': dim if dim != 'raw' else 'None',\n",
        "        'Best Params': param,\n",
        "        'Avg ARS': round(score, 4),\n",
        "        'Avg Time (s)': round(avg_time, 4)\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data).sort_values(by='Avg ARS', ascending=False).reset_index(drop=True)\n",
        "print(\"\\nTop Clustering Methods Summary:\")\n",
        "print(summary_df.head(top_n))\n",
        "\n",
        "# ---------- Plot 1: Avg Runtime ----------\n",
        "plt.figure(figsize=(14, 6))\n",
        "sorted_df = summary_df.head(top_n).sort_values(by='Avg Time (s)')\n",
        "sns.barplot(data=sorted_df, x='Clustering Method', y='Avg Time (s)', hue='Dim Reduction')\n",
        "plt.title(f'Average Runtime (Top {top_n} Methods)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------- Plot 2: Avg ARS by Method ----------\n",
        "plt.figure(figsize=(14, 6))\n",
        "sorted_df_score = summary_df.head(top_n).sort_values(by='Avg ARS', ascending=False)\n",
        "sns.barplot(data=sorted_df_score, x='Clustering Method', y='Avg ARS', hue='Dim Reduction')\n",
        "plt.title(f'Average ARS (Top {top_n} Methods)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZjY2zNV3vAyJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}