{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241443f-2ac6-4e3a-b6a9-a4d7868ed627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "# For parsing URLs:\n",
    "from urllib.parse import quote_plus\n",
    "import nltk\n",
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae13aa-fe51-4c9e-aaf7-0f9eae375a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cc_index(url, index_name):\n",
    "    \"\"\"\n",
    "    Search the Common Crawl Index for a given URL.\n",
    " \n",
    "    This function queries the Common Crawl Index <a href=\"https://www.jcchouinard.com/api/\">API</a> to find records related to the specified URL. \n",
    "    It uses the index specified by `index_name` to retrieve the data and returns a list of JSON objects, \n",
    "    each representing a record from the index.\n",
    " \n",
    "    Arguments:\n",
    "        url (str): The URL to search for in the Common Crawl Index.\n",
    "        index_name (str): The name of the Common Crawl Index to search (e.g., \"CC-MAIN-2024-10\").\n",
    " \n",
    "    Returns:\n",
    "        list: A list of JSON objects representing records found in the Common Crawl Index. \n",
    "              Returns None if the request fails or no records are found.\n",
    " \n",
    "    Example:\n",
    "        >>> search_cc_index(\"example.com\", \"CC-MAIN-2024-10\")\n",
    "        [{...}, {...}, ...]\n",
    "    \"\"\"\n",
    "    encoded_url = quote_plus(url)\n",
    "    index_url = f'http://index.commoncrawl.org/{index_name}-index?url={encoded_url}&output=json'\n",
    "    response = requests.get(index_url)\n",
    " \n",
    "    if response.status_code == 200:\n",
    "        records = response.text.strip().split('\\n')\n",
    "        return [json.loads(record) for record in records]\n",
    "    else:\n",
    "        return None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67b1b1-f49b-4bf1-8f09-7eb297bfa552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Regular expression to detect non-Latin characters\n",
    "non_latin_pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "\n",
    "allowed_domains = {'com', 'gov', 'edu', 'co', 'uk', 'net', 'mil', 'ai', 'ca'}\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en' and not non_latin_pattern.search(text)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_latin_not_english(text):\n",
    "    try:\n",
    "        return detect(text) != 'en' and not non_latin_pattern.search(text)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_last_domain_part(url:str):\n",
    "    return url.split(\"/\")[2].split(\".\")[-1]\n",
    "\n",
    "def is_error_response(input:str):\n",
    "    block_words = {\"404\"}\n",
    "    input = input.lower()\n",
    "    words = input.split()\n",
    "    for word in words:\n",
    "        if word in block_words:\n",
    "            return True\n",
    "    if input.find(\"no response\") >=0:\n",
    "        return True\n",
    "    if input.find(\"not found\") >=0:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def extract_english_files(warc_file):\n",
    "    \"\"\"\n",
    "    Returns a list of dictionaries with url description and title keys from \n",
    "    english web pges in a WARC file\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    results = []\n",
    "    with open(warc_file, 'rb') as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            if record.rec_type == 'response' and 'text/html' in record.http_headers.get('Content-Type', ''):\n",
    "                payload = record.content_stream().read()\n",
    "                soup = BeautifulSoup(payload, 'html.parser')\n",
    "                \n",
    "                html_tag = soup.find('html')\n",
    "                if html_tag and html_tag.get('lang', '').startswith('en'):\n",
    "                    url = record.rec_headers.get('WARC-Target-URI')\n",
    "                    if not get_last_domain_part(url) in allowed_domains:\n",
    "                        continue\n",
    "                    title_tag = soup.find('title')\n",
    "                    title = title_tag.text.strip() if title_tag else None\n",
    "                    if title is None or is_error_response(title):\n",
    "                        continue\n",
    "                    og_desc_tag = soup.find('meta', attrs={'property': 'og:description'})\n",
    "                    description = og_desc_tag.get('content', '').strip() if og_desc_tag else None\n",
    "                    if description is None:\n",
    "                        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "                        description = meta_desc_tag.get('content', '').strip() if meta_desc_tag else 'No Description'\n",
    "                    \n",
    "                    if not is_english(title):\n",
    "                        continue\n",
    "                    if count%20 == 0:\n",
    "                        print(count)\n",
    "                    count += 1\n",
    "                    results.append({\"url\": url, \"description\": description, \"title\": title})\n",
    "    return results\n",
    "\n",
    "def extract_non_english_latin(warc_file):\n",
    "    \"\"\"\n",
    "    Extracts files that have latin charsets in the title but a language detector determines as non-english\n",
    "    This is good for exracting error pages, pages in non-english languages\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    results = []\n",
    "    with open(warc_file, 'rb') as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            if record.rec_type == 'response' and 'text/html' in record.http_headers.get('Content-Type', ''):\n",
    "                payload = record.content_stream().read()\n",
    "                soup = BeautifulSoup(payload, 'html.parser')\n",
    "                \n",
    "                html_tag = soup.find('html')\n",
    "                if html_tag and not html_tag.get('lang', '').startswith('en'):\n",
    "                    url = record.rec_headers.get('WARC-Target-URI')\n",
    "                    if not get_last_domain_part(url) in allowed_domains:\n",
    "                        continue\n",
    "                    title_tag = soup.find('title')\n",
    "                    title = title_tag.text.strip() if title_tag else None\n",
    "                    if title is None or is_error_response(title):\n",
    "                        continue\n",
    "                    og_desc_tag = soup.find('meta', attrs={'property': 'og:description'})\n",
    "                    description = og_desc_tag.get('content', '').strip() if og_desc_tag else None\n",
    "                    if description is None:\n",
    "                        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "                        description = meta_desc_tag.get('content', '').strip() if meta_desc_tag else 'No Description'\n",
    "                    \n",
    "                    if not is_latin_not_english(title):\n",
    "                        continue\n",
    "                    if count%20 == 0:\n",
    "                        print(count)\n",
    "                    count += 1\n",
    "                    if count > 800:\n",
    "                        break\n",
    "                    results.append({\"url\": url, \"description\": description, \"title\": title})\n",
    "    return results    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d158a-acdb-4e91-8edc-e9dd7198d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = extract_english_files('/Users/Rrando/crawl/out/CC-MAIN-20250218081924-20250218111924-00893.warc.gz')\n",
    "cc_corpus = pd.DataFrame(r)\n",
    "cc_corpus.to_csv(\"../data/external/common_crawl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a61fac-b3d7-41f6-8f96-3b5683715451",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = extract_non_english_latin('/Users/Rrando/crawl/out/CC-MAIN-20250218081924-20250218111924-00893.warc.gz')\n",
    "cc_corpus = pd.DataFrame(r)\n",
    "cc_corpus.to_csv(\"../data/external/common_crawl_non_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc4277-c415-4d36-9406-81d3f9e88209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
