{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241443f-2ac6-4e3a-b6a9-a4d7868ed627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "# For parsing URLs:\n",
    "from urllib.parse import quote_plus\n",
    "import nltk\n",
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae13aa-fe51-4c9e-aaf7-0f9eae375a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cc_index(url, index_name):\n",
    "    \"\"\"\n",
    "    Search the Common Crawl Index for a given URL.\n",
    " \n",
    "    This function queries the Common Crawl Index <a href=\"https://www.jcchouinard.com/api/\">API</a> to find records related to the specified URL. \n",
    "    It uses the index specified by `index_name` to retrieve the data and returns a list of JSON objects, \n",
    "    each representing a record from the index.\n",
    " \n",
    "    Arguments:\n",
    "        url (str): The URL to search for in the Common Crawl Index.\n",
    "        index_name (str): The name of the Common Crawl Index to search (e.g., \"CC-MAIN-2024-10\").\n",
    " \n",
    "    Returns:\n",
    "        list: A list of JSON objects representing records found in the Common Crawl Index. \n",
    "              Returns None if the request fails or no records are found.\n",
    " \n",
    "    Example:\n",
    "        >>> search_cc_index(\"example.com\", \"CC-MAIN-2024-10\")\n",
    "        [{...}, {...}, ...]\n",
    "    \"\"\"\n",
    "    encoded_url = quote_plus(url)\n",
    "    index_url = f'http://index.commoncrawl.org/{index_name}-index?url={encoded_url}&output=json'\n",
    "    response = requests.get(index_url)\n",
    " \n",
    "    if response.status_code == 200:\n",
    "        records = response.text.strip().split('\\n')\n",
    "        return [json.loads(record) for record in records]\n",
    "    else:\n",
    "        return None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d1119-7eea-463e-bead-e57ad47aba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmarco = pd.read_table(\"../data/external/msmarco-tiny.tsv\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa82c87-e1eb-48ba-9731-7c61f9d1f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL you want to look up in the Common Crawl index\n",
    "target_urls = msmarco[1].to_list()[:20]\n",
    "\n",
    "# list of indexes https://commoncrawl.org/get-started\n",
    "indexes  = ['CC-MAIN-2020-10', 'CC-MAIN-2021-31'] #['CC-MAIN-2022-05', 'CC-MAIN-2024-33','CC-MAIN-2024-30','CC-MAIN-2024-26']\n",
    " \n",
    "record_dfs = []\n",
    "for target_url in target_urls:\n",
    "    # Fetch each index and store into a dataframe\n",
    "    for index_name in indexes:\n",
    "        print('Running: ', index_name)\n",
    "        records = search_cc_index(target_url,index_name)\n",
    "        record_df = pd.DataFrame(records)\n",
    "        record_df['index_name'] = index_name\n",
    "        record_dfs.append(record_df)\n",
    " \n",
    "# Combine individual dataframes\n",
    "all_records_df = pd.concat(record_dfs)\n",
    "all_records_df = all_records_df.sort_values(by='index_name', ascending=False)\n",
    "all_records_df = all_records_df.reset_index()\n",
    " \n",
    "# Create columns where to store data later\n",
    "all_records_df['success_status'] = 'not processed'\n",
    "all_records_df['html'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a5a97-0c71-43e3-b897-9936b538824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6cafff-3563-4b91-94d4-d51f539a36da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805dc85b-fc42-466b-90af-12d7185d99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38f10b-6cff-4408-9694-342329234eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tfds.as_dataframe(ds.take(10), ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a67b1b1-f49b-4bf1-8f09-7eb297bfa552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Regular expression to detect non-Latin characters\n",
    "non_latin_pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "\n",
    "allowed_domains = {'com', 'gov', 'edu', 'co', 'uk', 'net', 'mil', 'ai', 'ca'}\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en' and not non_latin_pattern.search(text)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def is_latin_not_english(text):\n",
    "    try:\n",
    "        return detect(text) != 'en' and not non_latin_pattern.search(text)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_last_domain_part(url:str):\n",
    "    return url.split(\"/\")[2].split(\".\")[-1]\n",
    "\n",
    "def is_error_response(input:str):\n",
    "    block_words = {\"404\"}\n",
    "    input = input.lower()\n",
    "    words = input.split()\n",
    "    for word in words:\n",
    "        if word in block_words:\n",
    "            return True\n",
    "    if input.find(\"no response\") >=0:\n",
    "        return True\n",
    "    if input.find(\"not found\") >=0:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def extract_english_files(warc_file):\n",
    "    count = 0\n",
    "\n",
    "    results = []\n",
    "    with open(warc_file, 'rb') as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            if record.rec_type == 'response' and 'text/html' in record.http_headers.get('Content-Type', ''):\n",
    "                payload = record.content_stream().read()\n",
    "                soup = BeautifulSoup(payload, 'html.parser')\n",
    "                \n",
    "                html_tag = soup.find('html')\n",
    "                if html_tag and html_tag.get('lang', '').startswith('en'):\n",
    "                    url = record.rec_headers.get('WARC-Target-URI')\n",
    "                    if not get_last_domain_part(url) in allowed_domains:\n",
    "                        continue\n",
    "                    title_tag = soup.find('title')\n",
    "                    title = title_tag.text.strip() if title_tag else None\n",
    "                    if title is None or is_error_response(title):\n",
    "                        continue\n",
    "                    og_desc_tag = soup.find('meta', attrs={'property': 'og:description'})\n",
    "                    description = og_desc_tag.get('content', '').strip() if og_desc_tag else None\n",
    "                    if description is None:\n",
    "                        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "                        description = meta_desc_tag.get('content', '').strip() if meta_desc_tag else 'No Description'\n",
    "                    \n",
    "                    if not is_english(title):\n",
    "                        continue\n",
    "                    if count%20 == 0:\n",
    "                        print(count)\n",
    "                    count += 1\n",
    "                    results.append({\"url\": url, \"description\": description, \"title\": title})\n",
    "    return results\n",
    "\n",
    "def extract_non_english_latin(warc_file):\n",
    "    count = 0\n",
    "    results = []\n",
    "    with open(warc_file, 'rb') as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            if record.rec_type == 'response' and 'text/html' in record.http_headers.get('Content-Type', ''):\n",
    "                payload = record.content_stream().read()\n",
    "                soup = BeautifulSoup(payload, 'html.parser')\n",
    "                \n",
    "                html_tag = soup.find('html')\n",
    "                if html_tag and not html_tag.get('lang', '').startswith('en'):\n",
    "                    url = record.rec_headers.get('WARC-Target-URI')\n",
    "                    if not get_last_domain_part(url) in allowed_domains:\n",
    "                        continue\n",
    "                    title_tag = soup.find('title')\n",
    "                    title = title_tag.text.strip() if title_tag else None\n",
    "                    if title is None or is_error_response(title):\n",
    "                        continue\n",
    "                    og_desc_tag = soup.find('meta', attrs={'property': 'og:description'})\n",
    "                    description = og_desc_tag.get('content', '').strip() if og_desc_tag else None\n",
    "                    if description is None:\n",
    "                        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "                        description = meta_desc_tag.get('content', '').strip() if meta_desc_tag else 'No Description'\n",
    "                    \n",
    "                    if not is_latin_not_english(title):\n",
    "                        continue\n",
    "                    if count%20 == 0:\n",
    "                        print(count)\n",
    "                    count += 1\n",
    "                    if count > 800:\n",
    "                        break\n",
    "                    results.append({\"url\": url, \"description\": description, \"title\": title})\n",
    "    return results    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d158a-acdb-4e91-8edc-e9dd7198d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = extract_english_files('/Users/Rrando/crawl/out/CC-MAIN-20250218081924-20250218111924-00893.warc.gz')\n",
    "cc_corpus = pd.DataFrame(r)\n",
    "cc_corpus.to_csv(\"../data/external/common_crawl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a61fac-b3d7-41f6-8f96-3b5683715451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "100\n",
      "120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n",
      "460\n",
      "480\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n",
      "540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "580\n",
      "600\n",
      "620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640\n",
      "660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/lt0dyzf14y93d8k18k68_4fm0000gn/T/ipykernel_32957/2785358724.py:84: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  soup = BeautifulSoup(payload, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680\n",
      "700\n",
      "720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "760\n",
      "780\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "r = extract_non_english_latin('/Users/Rrando/crawl/out/CC-MAIN-20250218081924-20250218111924-00893.warc.gz')\n",
    "cc_corpus = pd.DataFrame(r)\n",
    "cc_corpus.to_csv(\"../data/external/common_crawl_non_english.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "704588fb-8fb7-47ed-8167-04a2c596a6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'description', 'title'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_corpus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56965981-91c2-479c-9369-1b873b514e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Index(['Unnamed: 0.1', 'Unnamed: 0', 'input_titles', 'input_keywords',\n",
    "       'input_description', 'output', 'output_orig', 'before_none'],\n",
    "      dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06bdc1b-4f97-4d37-8cb0-819786cd3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_corpus[\"input_titles\"] = cc_corpus.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d123409-baa1-49ad-bacd-5bc70e26dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_corpus[\"input_keywords\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0dda8d9-2c55-446c-8128-e81ac55c2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_corpus[\"input_description\"] = cc_corpus.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbc2cd73-3944-4fd4-83d2-7faa848d3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_corpus.output = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2792e35a-82d8-4b24-be87-f18d2d9216de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_corpus = cc_corpus.drop(['description', 'input_keywords', 'title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b70f3cf5-998b-4902-9e30-ce7395567963",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_corpus[\"output\"] = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29e087b9-66af-42ec-8d98-2484058a46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_corpus.to_csv(\"common_corpus_noise_none_3_12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a27c51-d314-4456-aa57-c527212cca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_domain_part(\"https://cnn.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d23f29af-f2d6-4779-b015-602492a8f8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://24bet-ind.com/easy-access-to-24-betting...</td>\n",
       "      <td>No Description</td>\n",
       "      <td>game guide - 24 betting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://4strokeswimming.co.uk/</td>\n",
       "      <td>No Description</td>\n",
       "      <td>4strokeswimming.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://8xxx.net/category/ChangingRoom/</td>\n",
       "      <td></td>\n",
       "      <td>ChangingRoom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://agencia-mexico.com/details.php?gid=3036...</td>\n",
       "      <td>Isabel Madow, Mark Tacher, MarÃ­a Rojo, Julio ...</td>\n",
       "      <td>ISABEL MADOW + POSADA - CLAQUETAZO CINTA LA PRIMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://alpha.2findlocal.com/sitemap_2_14067.html</td>\n",
       "      <td>Free Advertising on 2FINDLOCAL.COM</td>\n",
       "      <td>Sitemap | 2FINDLOCAL.COM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>https://www.dekowerk.net/online-shop/carr%C3%A...</td>\n",
       "      <td>Unsere Produktlinie CARRÉ bietet Ihnen eine um...</td>\n",
       "      <td>Maison - dekowerk | Premium Design-Artikel aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>https://www.delfiero.net/eshop/scheda.asp?id=1...</td>\n",
       "      <td>No Description</td>\n",
       "      <td>Delfiero s.r.l.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>https://www.deporclub.com/horarios/gimnasia-2</td>\n",
       "      <td>Horario Gimnasia 2</td>\n",
       "      <td>Horario Gimnasia 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>https://www.derooysteeldoors.com/voorbeelden-s...</td>\n",
       "      <td>In deze woning in Ede hebben we in de woonkame...</td>\n",
       "      <td>Eenheid door stalen taatsdeuren - De Rooy Stee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>https://www.deutscherandhackett.com/auction/18...</td>\n",
       "      <td>No Description</td>\n",
       "      <td>CORNFLOWERS AND MELON, c.1970 | Deutscher and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "0    http://24bet-ind.com/easy-access-to-24-betting...   \n",
       "1                        http://4strokeswimming.co.uk/   \n",
       "2               http://8xxx.net/category/ChangingRoom/   \n",
       "3    http://agencia-mexico.com/details.php?gid=3036...   \n",
       "4     http://alpha.2findlocal.com/sitemap_2_14067.html   \n",
       "..                                                 ...   \n",
       "795  https://www.dekowerk.net/online-shop/carr%C3%A...   \n",
       "796  https://www.delfiero.net/eshop/scheda.asp?id=1...   \n",
       "797      https://www.deporclub.com/horarios/gimnasia-2   \n",
       "798  https://www.derooysteeldoors.com/voorbeelden-s...   \n",
       "799  https://www.deutscherandhackett.com/auction/18...   \n",
       "\n",
       "                                           description  \\\n",
       "0                                       No Description   \n",
       "1                                       No Description   \n",
       "2                                                        \n",
       "3    Isabel Madow, Mark Tacher, MarÃ­a Rojo, Julio ...   \n",
       "4                   Free Advertising on 2FINDLOCAL.COM   \n",
       "..                                                 ...   \n",
       "795  Unsere Produktlinie CARRÉ bietet Ihnen eine um...   \n",
       "796                                     No Description   \n",
       "797                                 Horario Gimnasia 2   \n",
       "798  In deze woning in Ede hebben we in de woonkame...   \n",
       "799                                     No Description   \n",
       "\n",
       "                                                 title  \n",
       "0                              game guide - 24 betting  \n",
       "1                                4strokeswimming.co.uk  \n",
       "2                                         ChangingRoom  \n",
       "3    ISABEL MADOW + POSADA - CLAQUETAZO CINTA LA PRIMA  \n",
       "4                             Sitemap | 2FINDLOCAL.COM  \n",
       "..                                                 ...  \n",
       "795  Maison - dekowerk | Premium Design-Artikel aus...  \n",
       "796                                    Delfiero s.r.l.  \n",
       "797                                 Horario Gimnasia 2  \n",
       "798  Eenheid door stalen taatsdeuren - De Rooy Stee...  \n",
       "799  CORNFLOWERS AND MELON, c.1970 | Deutscher and ...  \n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bc4277-c415-4d36-9406-81d3f9e88209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
