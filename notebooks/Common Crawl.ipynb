{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241443f-2ac6-4e3a-b6a9-a4d7868ed627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "# For parsing URLs:\n",
    "from urllib.parse import quote_plus\n",
    "import nltk\n",
    "from langdetect import detect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae13aa-fe51-4c9e-aaf7-0f9eae375a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cc_index(url, index_name):\n",
    "    \"\"\"\n",
    "    Search the Common Crawl Index for a given URL.\n",
    " \n",
    "    This function queries the Common Crawl Index <a href=\"https://www.jcchouinard.com/api/\">API</a> to find records related to the specified URL. \n",
    "    It uses the index specified by `index_name` to retrieve the data and returns a list of JSON objects, \n",
    "    each representing a record from the index.\n",
    " \n",
    "    Arguments:\n",
    "        url (str): The URL to search for in the Common Crawl Index.\n",
    "        index_name (str): The name of the Common Crawl Index to search (e.g., \"CC-MAIN-2024-10\").\n",
    " \n",
    "    Returns:\n",
    "        list: A list of JSON objects representing records found in the Common Crawl Index. \n",
    "              Returns None if the request fails or no records are found.\n",
    " \n",
    "    Example:\n",
    "        >>> search_cc_index(\"example.com\", \"CC-MAIN-2024-10\")\n",
    "        [{...}, {...}, ...]\n",
    "    \"\"\"\n",
    "    encoded_url = quote_plus(url)\n",
    "    index_url = f'http://index.commoncrawl.org/{index_name}-index?url={encoded_url}&output=json'\n",
    "    response = requests.get(index_url)\n",
    " \n",
    "    if response.status_code == 200:\n",
    "        records = response.text.strip().split('\\n')\n",
    "        return [json.loads(record) for record in records]\n",
    "    else:\n",
    "        return None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d1119-7eea-463e-bead-e57ad47aba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "msmarco = pd.read_table(\"../data/external/msmarco-tiny.tsv\", header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa82c87-e1eb-48ba-9731-7c61f9d1f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL you want to look up in the Common Crawl index\n",
    "target_urls = msmarco[1].to_list()[:20]\n",
    "\n",
    "# list of indexes https://commoncrawl.org/get-started\n",
    "indexes  = ['CC-MAIN-2020-10', 'CC-MAIN-2021-31'] #['CC-MAIN-2022-05', 'CC-MAIN-2024-33','CC-MAIN-2024-30','CC-MAIN-2024-26']\n",
    " \n",
    "record_dfs = []\n",
    "for target_url in target_urls:\n",
    "    # Fetch each index and store into a dataframe\n",
    "    for index_name in indexes:\n",
    "        print('Running: ', index_name)\n",
    "        records = search_cc_index(target_url,index_name)\n",
    "        record_df = pd.DataFrame(records)\n",
    "        record_df['index_name'] = index_name\n",
    "        record_dfs.append(record_df)\n",
    " \n",
    "# Combine individual dataframes\n",
    "all_records_df = pd.concat(record_dfs)\n",
    "all_records_df = all_records_df.sort_values(by='index_name', ascending=False)\n",
    "all_records_df = all_records_df.reset_index()\n",
    " \n",
    "# Create columns where to store data later\n",
    "all_records_df['success_status'] = 'not processed'\n",
    "all_records_df['html'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561a5a97-0c71-43e3-b897-9936b538824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6cafff-3563-4b91-94d4-d51f539a36da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805dc85b-fc42-466b-90af-12d7185d99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38f10b-6cff-4408-9694-342329234eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tfds.as_dataframe(ds.take(10), ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a67b1b1-f49b-4bf1-8f09-7eb297bfa552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Regular expression to detect non-Latin characters\n",
    "non_latin_pattern = re.compile(r'[^\\x00-\\x7F]+')\n",
    "\n",
    "allowed_domains = {'com', 'gov', 'edu', 'co', 'uk', 'net', 'mil', 'ai', 'ca'}\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en' and not non_latin_pattern.search(text)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_last_domain_part(url:str):\n",
    "    return url.split(\"/\")[2].split(\".\")[-1]\n",
    "\n",
    "def is_error_response(input:str):\n",
    "    block_words = {\"404\"}\n",
    "    input = input.lower()\n",
    "    words = input.split()\n",
    "    for word in words:\n",
    "        if word in block_words:\n",
    "            return True\n",
    "    if input.find(\"no response\") >=0:\n",
    "        return True\n",
    "    if input.find(\"not found\") >=0:\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def extract_english_files(warc_file):\n",
    "    count = 0\n",
    "\n",
    "    results = []\n",
    "    with open(warc_file, 'rb') as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            if record.rec_type == 'response' and 'text/html' in record.http_headers.get('Content-Type', ''):\n",
    "                payload = record.content_stream().read()\n",
    "                soup = BeautifulSoup(payload, 'html.parser')\n",
    "                \n",
    "                html_tag = soup.find('html')\n",
    "                if html_tag and html_tag.get('lang', '').startswith('en'):\n",
    "                    url = record.rec_headers.get('WARC-Target-URI')\n",
    "                    if not get_last_domain_part(url) in allowed_domains:\n",
    "                        continue\n",
    "                    title_tag = soup.find('title')\n",
    "                    title = title_tag.text.strip() if title_tag else None\n",
    "                    if title is None or is_error_response(title):\n",
    "                        continue\n",
    "                    og_desc_tag = soup.find('meta', attrs={'property': 'og:description'})\n",
    "                    description = og_desc_tag.get('content', '').strip() if og_desc_tag else None\n",
    "                    if description is None:\n",
    "                        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "                        description = meta_desc_tag.get('content', '').strip() if meta_desc_tag else 'No Description'\n",
    "                    \n",
    "                    if not is_english(title):\n",
    "                        continue\n",
    "                    if count%20 == 0:\n",
    "                        print(count)\n",
    "                    count += 1\n",
    "                    results.append({\"url\": url, \"description\": description, \"title\": title})\n",
    "    return results\n",
    "                    \n",
    "r = extract_english_files('/Users/Rrando/crawl/out/CC-MAIN-20250218081924-20250218111924-00893.warc.gz')\n",
    "cc_corpus = pd.DataFrame(r)\n",
    "cc_corpus.to_csv(\"../data/external/common_crawl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56965981-91c2-479c-9369-1b873b514e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06bdc1b-4f97-4d37-8cb0-819786cd3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a27c51-d314-4456-aa57-c527212cca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_domain_part(\"https://cnn.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f29af-f2d6-4779-b015-602492a8f8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
